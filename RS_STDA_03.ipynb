{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7b25c1c9-80de-402a-a10c-026d7955da39",
      "metadata": {
        "id": "7b25c1c9-80de-402a-a10c-026d7955da39"
      },
      "source": [
        "# Research Skills: Spatiotemporal Data Analysis\n",
        "## Module 3 - More Time Series Forecasting, Time Series Regression and Classification\n",
        "\n",
        "Sharon Ong, Department of Cognitive Science and Artificial Intelligence – Tilburg University\n",
        "\n",
        "You will work with three time-series datasets for forecasting:\n",
        "1. The monthly totals of international airline passengers (in thousands) from January 1949 to December 1960.\n",
        "2. The US Macroeconomic Data for Q1, 1959 to Q3,2009.\n",
        "3. This mulitvariate time series dataset contains various US macroeconomic variables from 1947 to 1962 that are known to be highly collinear.\n",
        "\n",
        "You will work also work with two datasets for time series classification:\n",
        "1. The twelve monthly electrical power demand time series from Italy. The classification task is to distinguish days from Oct to March (inclusive) from April to September.\n",
        "2. The Basic Motions dataset. The data was generated as part of a student project where four students performed four activities whilst wearing a smart watch. The watch collects 3D accelerometer and a 3D gyroscope. It consists of four classes, which are walking, resting, running and badminton\n",
        "\n",
        "The Entry level exercises include\n",
        "1. Vector Autogressions\n",
        "2. Forecasting with Exogenous Variables\n",
        "3. Time Series Classification\n",
        "\n",
        "The advanced level exerises include\n",
        "4. Pipelines and Cross validation with Time Series"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7877cf1c-e173-48bb-a268-c17eec5c8069",
      "metadata": {
        "id": "7877cf1c-e173-48bb-a268-c17eec5c8069"
      },
      "source": [
        "### 0. Setup\n",
        "Please specify in the next cell if you are working from Google Colab or from your own computer. Also indicate if you already have the statsmodels library installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ee4a12-221b-4efd-b058-e49b5a4cdfd3",
      "metadata": {
        "id": "d3ee4a12-221b-4efd-b058-e49b5a4cdfd3"
      },
      "outputs": [],
      "source": [
        "COLAB = True\n",
        "SKTIME_INSTALLED = False\n",
        "PDARIMA_INSTALLED = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05e20d5b-dc37-41fc-915b-503cd0c87491",
      "metadata": {
        "id": "05e20d5b-dc37-41fc-915b-503cd0c87491"
      },
      "source": [
        "Now run the following to set up the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f970952-9b52-4038-bd3b-ce86bc1ea2c4",
      "metadata": {
        "id": "5f970952-9b52-4038-bd3b-ce86bc1ea2c4"
      },
      "outputs": [],
      "source": [
        "if COLAB:\n",
        "#    from google.colab import drive\n",
        "#    drive.mount('/content/drive')\n",
        "#    # Load the contents of the directory\n",
        "    !ls\n",
        "#    # Change your working directory to the folder where you stored your files, e.g.\n",
        "#    %cd /content/drive/My Drive/Colab Notebooks/STDA\n",
        "\n",
        "if not SKTIME_INSTALLED:\n",
        "    !pip install sktime\n",
        "    !pip install statsmodels\n",
        "\n",
        "if not PDARIMA_INSTALLED:\n",
        "    !pip install pmdarima\n",
        "\n",
        "import itertools\n",
        "from os.path import join\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e152a11-06d3-42e3-9c1f-e9b0be1c5f8a",
      "metadata": {
        "id": "3e152a11-06d3-42e3-9c1f-e9b0be1c5f8a"
      },
      "source": [
        "# 1. Multivariate Forecasting with Vector Autoregression\n",
        "\n",
        "Multivariate forecasting involves predicting future values of multiple dependent variables based on their historical patterns and relationships.\n",
        "\n",
        "In univariate time series, we focus on only a single time-dependent variable. In contrast, a multivariate time series deals with multiple time-dependent variables. Each variable depends not only on its own past values but also has some dependency on other variables.\n",
        "For example, imagine a dataset that includes temperature, humidity, and air pressure measurements over time. Here, we have multiple variables (temperature, humidity, and air pressure) that interact with each other. Multivariate time series models consider these interdependencies to forecast future values for all variables simultaneously.\n",
        "\n",
        "We will use the US Macroeconomic Data for Q1, 1959 to Q3,2009. We will use 2 of the features in this dataset which are\n",
        "* realgdp - Real gross domestic product\n",
        "* realcons- Real personal consumption expenditures\n",
        "* realinv - Real gross private domestic investment\n",
        "* realgovt- Real federal consumption expenditures & gross investment    \n",
        "\n",
        "The following code loads and displays the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cfbc64b-456a-4fc5-9cc5-a15e8a5df62c",
      "metadata": {
        "id": "9cfbc64b-456a-4fc5-9cc5-a15e8a5df62c"
      },
      "outputs": [],
      "source": [
        "from sktime.datasets import load_macroeconomic\n",
        "from sktime.utils.plotting import plot_series\n",
        "\n",
        "y = load_macroeconomic()\n",
        "y.plot(subplots = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use two variables in this dataset, \"realgdp\" and \"realcons\".  The following code extracts the 2 features.  The VAR class assumes that the passed time series are stationary. Non-stationary or trending data can often be transformed to be stationary by first-differencing or some other method. Perform a stationary check by applying an Augmented Dickey-Fuller test on each of these variables.\n"
      ],
      "metadata": {
        "id": "HZObp-am8Lgh"
      },
      "id": "HZObp-am8Lgh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45048975-65be-4ca1-b799-15dc013fb984",
      "metadata": {
        "id": "45048975-65be-4ca1-b799-15dc013fb984"
      },
      "outputs": [],
      "source": [
        "from sktime.param_est.stationarity import StationarityADF\n",
        "y = y[[\"realgdp\",\"realcons\"]]\n",
        "#\n",
        "# Your codes goes here\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the time series which are not stationary, make them stationary.  "
      ],
      "metadata": {
        "id": "HSxxqFmL875u"
      },
      "id": "HSxxqFmL875u"
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Your codes goes here\n",
        "#"
      ],
      "metadata": {
        "id": "IOGzvaoR87Yh"
      },
      "id": "IOGzvaoR87Yh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code splits the code into training and test and runs and evaluate a vector autoregression with a constant trend and up to 4 lags.  "
      ],
      "metadata": {
        "id": "9hc6PPI1K2kj"
      },
      "id": "9hc6PPI1K2kj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce596146-033c-4f93-9e7c-796026e8ce44",
      "metadata": {
        "id": "ce596146-033c-4f93-9e7c-796026e8ce44"
      },
      "outputs": [],
      "source": [
        "from sktime.forecasting.var import VAR\n",
        "from sktime.split import temporal_train_test_split\n",
        "from sktime.utils.plotting import plot_series\n",
        "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
        "import numpy as np\n",
        "\n",
        "test_size = 48\n",
        "fh = np.arange(1, test_size+1)\n",
        "\n",
        "y_train1, y_test1 = temporal_train_test_split(y, test_size=test_size)\n",
        "\n",
        "# Initialize a vector autoregression with\n",
        "forecaster = VAR(trend= 'n', maxlags =  4)\n",
        "forecaster.fit(y_train1)\n",
        "print(forecaster.is_fitted)\n",
        "print(forecaster.get_params())\n",
        "\n",
        "# predict the results\n",
        "y_pred = forecaster.predict(fh=fh)\n",
        "print(forecaster.get_fitted_params())\n",
        "\n",
        "# evaluate the results\n",
        "print('MAPE for VAR: ', mean_absolute_percentage_error(y_test1, y_pred, symmetric=False))\n",
        "# plot the results\n",
        "plot_series(y_train1[\"realgdp\"] , y_test1[\"realgdp\"], y_pred[\"realgdp\"] ,labels=[\"y_train\", \"y_val\", \"y_pred\"], title = 'realgdp')\n",
        "plot_series(y_train1[\"realcons\"] , y_test1[\"realcons\"], y_pred[\"realcons\"] ,labels=[\"y_train\", \"y_val\", \"y_pred\"], title = 'realcons')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VARMAX is the vector form of ARIMA (with exogenous variables). Fit a VARMAX and predict and evaluate the results."
      ],
      "metadata": {
        "id": "McdZCAUcLDGD"
      },
      "id": "McdZCAUcLDGD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12855494-9a45-4c9c-b832-073818fb852a",
      "metadata": {
        "id": "12855494-9a45-4c9c-b832-073818fb852a"
      },
      "outputs": [],
      "source": [
        "from sktime.forecasting.varmax import VARMAX\n",
        "import numpy as np\n",
        "\n",
        "test_size = 60\n",
        "fh = np.arange(1, test_size+1)\n",
        "\n",
        "y_train1, y_test1 = temporal_train_test_split(y,test_size=test_size)\n",
        "\n",
        "\n",
        "forecaster = VARMAX(suppress_warnings=True)\n",
        "\n",
        "\n",
        "#\n",
        "# Your code goes here\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Granger causality is a statistical hypothesis test that assesses whether one time series can be used to predict another time series. The following code runs the grangers causality test, If the pvalue is less than 0.05 then it means that one time series can be used to predict another.\n"
      ],
      "metadata": {
        "id": "7vtCAtuZAbBD"
      },
      "id": "7vtCAtuZAbBD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5d382ca-0afa-4261-a052-2c7f1f4aebb6",
      "metadata": {
        "id": "e5d382ca-0afa-4261-a052-2c7f1f4aebb6"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "\n",
        "gc_res = grangercausalitytests(y, 12)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Univariate forecasting with exogenous variables\n",
        "\n",
        "This mulitvariate time series dataset contains various US macroeconomic variables from 1947 to 1962 that are known to be highly collinear. We will forecast total employement (TOTEMP) with it's lag and 5 other variables:\n",
        "\n",
        "* GNPDEFL - Gross national product deflator\n",
        "* GNP - Gross national product\n",
        "* UNEMP - Number of unemployed\n",
        "* ARMED - Size of armed forces\n",
        "* POP - Population\n",
        "\n",
        "The following code run a ARIMA model with exogeenous variables. Compare this model with an ARIMA model without exogeenous variables."
      ],
      "metadata": {
        "id": "unpon-OFAfyX"
      },
      "id": "unpon-OFAfyX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a38a0cfc-057e-44c2-9c29-da895f6fa9f6",
      "metadata": {
        "id": "a38a0cfc-057e-44c2-9c29-da895f6fa9f6"
      },
      "outputs": [],
      "source": [
        "from sktime.datasets import load_longley\n",
        "from sktime.forecasting.base import ForecastingHorizon\n",
        "from sktime.split import temporal_train_test_split\n",
        "from sktime.utils.plotting import plot_series\n",
        "from sktime.forecasting.arima import AutoARIMA\n",
        "from sktime.forecasting.sarimax import SARIMAX\n",
        "\n",
        "\n",
        "y, X = load_longley()\n",
        "y_train, y_test, X_train, X_test = temporal_train_test_split(y=y, X=X, test_size=6)\n",
        "fh = ForecastingHorizon(y_test.index, is_relative=False)\n",
        "\n",
        "forecaster = SARIMAX(order=(1, 0, 0), trend=\"t\", seasonal_order=(1, 0, 0, 6))\n",
        "forecaster.fit(y=y_train, X=X_train)\n",
        "y_pred = forecaster.predict(fh=fh, X=X_test)\n",
        "print('MAPE for SARIMAX: ', mean_absolute_percentage_error(y_test, y_pred, symmetric=False))\n",
        "\n",
        "#\n",
        "# Your code goes here\n",
        "#\n",
        "# try univariate forcasting with ARIMA without exogenous variables"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`AutoARIMA` is another function which supports forecasting with exogenous variables.  "
      ],
      "metadata": {
        "id": "Jp4vtRkJ_KWT"
      },
      "id": "Jp4vtRkJ_KWT"
    },
    {
      "cell_type": "code",
      "source": [
        "forecaster = AutoARIMA(suppress_warnings=True)\n",
        "forecaster.fit(y=y_train, X=X_train)\n",
        "y_pred = forecaster.predict(fh=fh, X=X_test)\n",
        "\n",
        "print('MAPE for AutoARIMA : ', mean_absolute_percentage_error(y_test, y_pred, symmetric=False))"
      ],
      "metadata": {
        "id": "Ui4jF_e6_KDa"
      },
      "id": "Ui4jF_e6_KDa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the residual errors for the SARIMAX solution with a residual plot, qqplot and histogram plot.  "
      ],
      "metadata": {
        "id": "OGWjLUrsNtAk"
      },
      "id": "OGWjLUrsNtAk"
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Your code goes here\n",
        "#\n"
      ],
      "metadata": {
        "id": "dALM_RsMLxKK"
      },
      "id": "dALM_RsMLxKK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1acce038-66ab-4930-9872-66239a3e73e6",
      "metadata": {
        "id": "1acce038-66ab-4930-9872-66239a3e73e6"
      },
      "source": [
        "# 3. Time Series Classification.  \n",
        "\n",
        "# 3.1  Univariate Time Series Classification\n",
        "The following code loads the training and test data from the Italy Power Demaind dataset. The dataset is univaraite.\n",
        "\n",
        "THere are two classes, one for the period Oct to March (class 1) and from April to September (class 2). Display each time series, giving a different label for each different class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6aab6a4-cbdd-4f7a-b8ab-05998093db5e",
      "metadata": {
        "id": "d6aab6a4-cbdd-4f7a-b8ab-05998093db5e"
      },
      "outputs": [],
      "source": [
        "from sktime.datasets import load_basic_motions\n",
        "from sktime.datasets import load_italy_power_demand\n",
        "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
        "\n",
        "from sktime.dists_kernels import FlatDist, ScipyDist\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0bf1950-f68b-4722-9c80-90ed4f1c5e75",
      "metadata": {
        "id": "f0bf1950-f68b-4722-9c80-90ed4f1c5e75"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = load_italy_power_demand(split=\"train\", return_type=\"numpy2d\")\n",
        "X_test, y_test = load_italy_power_demand(split=\"test\", return_type=\"numpy2d\")\n",
        "\n",
        "\n",
        "#\n",
        "# Your code goes here\n",
        "#\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can train a KNN Time Series Classifier and evaluate the accuracy."
      ],
      "metadata": {
        "id": "EKb8HSIhDRfK"
      },
      "id": "EKb8HSIhDRfK"
    },
    {
      "cell_type": "code",
      "source": [
        "eucl_dist = FlatDist(ScipyDist())\n",
        "clf = KNeighborsTimeSeriesClassifier(n_neighbors=3, distance=eucl_dist)\n",
        "clf.get_params()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "# for simplest evaluation, compare ground truth to predictions\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "tPzSt0D-B6Ru"
      },
      "id": "tPzSt0D-B6Ru",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and evaluate the dataset with the following classifiers\n",
        "\n",
        "* IndividualBOSS\n",
        "* RocketClassifier\n",
        "* Catch22Classifier"
      ],
      "metadata": {
        "id": "1nlO5TXpFOq5"
      },
      "id": "1nlO5TXpFOq5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "555ab9a4-b687-43b1-ada2-38ab61a67c0e",
      "metadata": {
        "id": "555ab9a4-b687-43b1-ada2-38ab61a67c0e"
      },
      "outputs": [],
      "source": [
        "from sktime.classification.dictionary_based import IndividualBOSS\n",
        "from sktime.classification.feature_based import Catch22Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sktime.classification.kernel_based import RocketClassifier\n",
        "\n",
        "\n",
        "#\n",
        "# Your code goes here\n",
        "#\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2 Multivariate time series classification\n",
        "\n",
        "The following code runs the basic motions dataset and display each time series with its class.  The dataset consists of 6 variables from a 3D accelerometer and a 3D gyroscope. It consists of four classes, which are walking, resting, running and badminton\n"
      ],
      "metadata": {
        "id": "d_GUO5oKCAOt"
      },
      "id": "d_GUO5oKCAOt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbdbde8a-6ada-4c0e-929b-4afad910cc57",
      "metadata": {
        "id": "cbdbde8a-6ada-4c0e-929b-4afad910cc57"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = load_basic_motions(split=\"train\", return_type=\"numpy3D\")\n",
        "X_test, y_test = load_basic_motions(split=\"test\", return_type=\"numpy3D\")\n",
        "\n",
        "# individual time series for each variable\n",
        "for b in np.arange(0, X_train.shape[1]):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    for a in np.arange(0, len(y_train)):\n",
        "        if(y_train[a] == 'badminton'):\n",
        "            plt.plot(X_train[a,b],'g')\n",
        "        elif(y_train[a] == 'running'):\n",
        "            plt.plot(X_train[a,b],'r--')\n",
        "        elif(y_train[a] == 'standing'):\n",
        "            plt.plot(X_train[a,b],'b+-')\n",
        "        elif(y_train[a] == 'walking'):\n",
        "            plt.plot(X_train[a,b],'c.-')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some time series classifies only work from univariate classifiers. Try and train and evaluate the dataset with the following classifiers\n",
        "* KNeighborsTimeSeriesClassifier\n",
        "* IndividualBOSS\n",
        "* RocketClassifier\n",
        "* Catch22Classifier\n",
        "\n",
        "Do these classifiers all work with multivariate time series classification?"
      ],
      "metadata": {
        "id": "kcwWuOddN3if"
      },
      "id": "kcwWuOddN3if"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6e3a79-f78a-48fe-b6d5-43979f4ca013",
      "metadata": {
        "id": "9f6e3a79-f78a-48fe-b6d5-43979f4ca013"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Your code goes here\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Advanced Section: Cross validation and Pipelines with Time Series\n",
        "\n",
        "#4.1 Cross validation for Time Series Classification.\n",
        "\n",
        "You can apply cross validation on time series for classification in the same way as you do for classifiers and regressors in sklearn.  You can also tune hyperparameters using `GridSearchCV`. The following code loads the Italy power demand dataset.\n",
        "\n",
        "1. Initialize a `KNeighborsTimeSeriesClassifier`\n",
        "2. Perform 5-fold cross validation using with `cross_val_score'`.\n",
        "3. Split your data into training and testing.\n",
        "4. Perform hyperparameter tuning on the training set, with `n_neighbors = [1, 3, 5]` and with 5 folds using `GridSearchCV`.\n",
        "5. Fit the best parameters on the training set and evaluate on the test set."
      ],
      "metadata": {
        "id": "LIpv0BvSQjs3"
      },
      "id": "LIpv0BvSQjs3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ae2a34b-3103-4010-aaab-73af923a4fd2",
      "metadata": {
        "id": "1ae2a34b-3103-4010-aaab-73af923a4fd2"
      },
      "outputs": [],
      "source": [
        "# cross validation stuff\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
        "from sktime.classification.feature_based import SummaryClassifier\n",
        "\n",
        "X, y = load_italy_power_demand(return_type=\"numpy2d\")\n",
        "\n",
        "#\n",
        "# Your code goes here\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.2 Cross validation for time series forecasting\n",
        "\n",
        "In sktime there are three different types of temporal cross-validation splitters available:\n",
        "* SingleWindowSplitter, which is equivalent to a single temporal train-test-split\n",
        "* SlidingWindowSplitter, which is using a rolling window approach and “forgets” the oldest observations as we move more into the future\n",
        "* ExpandingWindowSplitter, which is using a expanding window approach and keep all observations in the training set as we move more into the future\n",
        "\n",
        "The following code calls a sliding window splitter."
      ],
      "metadata": {
        "id": "Jv7xy9AqCGEH"
      },
      "id": "Jv7xy9AqCGEH"
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.forecasting.model_evaluation import evaluate\n",
        "from sktime.forecasting.arima import ARIMA\n",
        "from sktime.datasets import load_shampoo_sales\n",
        "\n",
        "y = load_shampoo_sales()\n",
        "y_train, y_test = temporal_train_test_split(y=y, test_size=6)\n",
        "\n",
        "forecaster = ARIMA()\n",
        "cv = SlidingWindowSplitter(fh=6, window_length=12, step_length=1)\n",
        "\n",
        "plot_windows(cv=cv, y=y_train)\n",
        "\n",
        "out = evaluate(forecaster, cv, y=y)\n",
        "print(out)\n"
      ],
      "metadata": {
        "id": "p-GHMdFdFWtY"
      },
      "id": "p-GHMdFdFWtY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Pipelines for forecasting\n",
        "\n",
        "We can transform the time series (e.g. apply a log transformer, DeTrender) and before running a forecasting algorithm.\n",
        "\n",
        "1. Load the airline dataset and set the Forecasting Horizon to 36 months\n",
        "2. Split your data into training and test.\n",
        "3. Fit `ARIMA` or `AutoARIMA` on the training set with the appropriate hyperparameters and evaluate on the test set.\n",
        "4. Make an instance of the `Detrender`, run `fit_transform` on the training set.\n",
        "5.  Fit `ARIMA` or `AutoARIMA` on the result of `fit_transform`, on the training set with the appropriate hyperparameters. Make predictions up to the forecasting horizon. Perfomr an `inverse_transform` on the predicted results.\n",
        "6. Plot the predicted time series from No. 3 and No. 5 along with the training and test set.\n",
        "7. Apply a `Deseasonalizer` transformation together with the `Detrender` the data."
      ],
      "metadata": {
        "id": "ah0gVGdwDczW"
      },
      "id": "ah0gVGdwDczW"
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.transformations.series.detrend import Deseasonalizer, Detrender\n",
        "from sktime.datasets import load_airline\n",
        "from sktime.split import temporal_train_test_split\n",
        "from sktime.utils.plotting import plot_series\n",
        "from sktime.forecasting.arima import AutoARIMA\n",
        "\n",
        "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
        "import numpy as np\n",
        "\n",
        "#\n",
        "# Your code goes here\n",
        "#\n"
      ],
      "metadata": {
        "id": "dsSXrO5iYmiX"
      },
      "id": "dsSXrO5iYmiX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `TransformedTargetForecaster` create a pipeline to run the transformer and forecaster.  "
      ],
      "metadata": {
        "id": "dVQmU6ZYtiJh"
      },
      "id": "dVQmU6ZYtiJh"
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.forecasting.compose import TransformedTargetForecaster\n",
        "\n",
        "pipe_y = TransformedTargetForecaster(\n",
        "    steps=[\n",
        "        (\"detrend\", Detrender()),\n",
        "        (\"forecaster\", AutoARIMA(sp=12, suppress_warnings=True)),\n",
        "    ]\n",
        ")\n",
        "pipe_y.fit(y=y_train)\n",
        "y_pred = pipe_y.predict(fh=fh)\n",
        "plot_series(y_train, y_test, y_pred ,labels=[\"y_train\", \"test\", \"y_pred\"], title = 'with detrending ')"
      ],
      "metadata": {
        "id": "2SOVVFaWZGoy"
      },
      "id": "2SOVVFaWZGoy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run a pipeline which includes a `Detrender` and `Deseasonalizer` with `AutoARIMA` on the same dataset.  \n"
      ],
      "metadata": {
        "id": "5SLNPHQaA4A2"
      },
      "id": "5SLNPHQaA4A2"
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Your code goes here.\n",
        "#"
      ],
      "metadata": {
        "id": "wANE47g8BIbx"
      },
      "id": "wANE47g8BIbx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}